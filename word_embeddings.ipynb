{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RinadAkel/Word-Embeddings/blob/main/word_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLRfLrVXtUTI"
      },
      "source": [
        "© 2021 Zaka AI, Inc. All Rights Reserved"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caLGnxM2Ib_T"
      },
      "source": [
        "# Word Embeddings\n",
        "**Objective:** The goal from this exercise is to explore the Word2Vec technique for word embeddings and introduce Stanford's GloVe embedding as well. The libraries we will be using are `Gensim` for Word Embeddings Word2Vec and GloVe, `matplotlib` for visualization and `Scikit-Learn` for Principle Component Analysis models which are used for reducing dimensionality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dj1QPhssIfi8"
      },
      "source": [
        "## Learn Word2Vec Embedding using Gensim\n",
        "\n",
        "Word2Vec models require a lot of text, e.g. the entire Wikipedia corpus. However, we will demonstrate the principles using a small in-memory example of text.\n",
        "\n",
        "Each sentence must be tokenized (divided into words and prepared). The sentences could be text loaded into memory, or an iterator that progressively loads text, required for very large text corpora. \n",
        "\n",
        "There are many parameters on this constructor:\n",
        "\n",
        "*   **`size`**: (default 100) The number of dimensions of the embedding, e.g. the length of the dense vector to represent each token (word).\n",
        "*   **`window`**: (default 5) The maximum distance between a target word and words around the target word.\n",
        "*   **`min_count`**: (default 5) The minimum count of words to consider when training the model; words with an occurrence less than this count will be ignored.\n",
        "*   **`workers`**: (default 3) The number of threads to use while training.\n",
        "*   **`sg`**: (default 0 or CBOW) The training algorithm, either CBOW (0) or skip gram (1)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7pU1nY3LhkO"
      },
      "source": [
        "###Building and training a Word2Vec model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bY9k0TquxeHp"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# define training data\n",
        "sentences = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
        "\t\t\t['this', 'is', 'the', 'second', 'sentence'],\n",
        "\t\t\t['yet', 'another', 'sentence'],\n",
        "\t\t\t['one', 'more', 'sentence'],\n",
        "\t\t\t['and', 'the', 'final', 'sentence']]\n",
        "\n",
        "# train model\n",
        "model = Word2Vec(sentences, min_count=1)\n",
        "\n",
        "# summarize the loaded model\n",
        "print(model)\n",
        "\n",
        "# summarize vocabulary\n",
        "words = list(model.wv.vocab)\n",
        "print(words)\n",
        "\n",
        "# access vector for one word\n",
        "print(model['sentence'])\n",
        "\n",
        "# save model\n",
        "model.save('model.bin')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWjTGAuQC59K"
      },
      "source": [
        "# let's load the model and test it\n",
        "\n",
        "# load model\n",
        "new_model = Word2Vec.load('model.bin')\n",
        "print(new_model['this', 'is'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moU2ue2iCCGm"
      },
      "source": [
        "### Visualize Word Embedding\n",
        "\n",
        "After learning the word embedding for the text, it's nice to explore it with visualization. We can use classical projection methods to reduce the high-dimensional word vectors to two- dimensional plots and plot them on a graph. The visualizations can provide a qualitative diagnostic for our learned model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSr__Ji1Ilku"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "from sklearn.decomposition import PCA\n",
        "from matplotlib import pyplot\n",
        "\n",
        "# define training data\n",
        "sentences = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
        "\t\t\t['this', 'is', 'the', 'second', 'sentence'],\n",
        "\t\t\t['yet', 'another', 'sentence'],\n",
        "\t\t\t['one', 'more', 'sentence'],\n",
        "\t\t\t['and', 'the', 'final', 'sentence']]\n",
        "\n",
        "# train model\n",
        "model = Word2Vec(sentences, min_count=1)\n",
        "\n",
        "# fit a 2D PCA model to the vectors\n",
        "X = model[model.wv.vocab]\n",
        "pca = PCA(n_components=2) \t\t\t\t#reduce dimensionality to 2D\n",
        "result = pca.fit_transform(X) \t\t#2D model to plot\n",
        "\n",
        "# create a scatter plot of the projection\n",
        "# pull out the 2 dimensions as x and y\n",
        "pyplot.scatter(result[:, 0], result[:, 1])\n",
        "words = list(model.wv.vocab)\n",
        "\n",
        "# annotate the points on the graph with the words themselves\n",
        "for i, word in enumerate(words):\n",
        "\tpyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLxMShJHJQIs"
      },
      "source": [
        "## Google Word2Vec \n",
        "\n",
        "Instead of training your own word vectors (which requires a lot of RAM and compute power), you can simply use a pre-trained word embedding. Google has published a pre-trained Word2Vec model that was trained on Google news data (about 100 billion words). It contains 3 million words and phrases and was fit using 300-dimensional word vectors. It is a 1.53 Gigabyte file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLD7sHm1LX_U"
      },
      "source": [
        "# download Google's word embeddings\n",
        "!wget \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\" -O \"GoogleNews-vectors-negative300.bin.gz\" \n",
        "\n",
        "# unzip downloaded word embeddings\n",
        "!gunzip GoogleNews-vectors-negative300.bin.gz  \n",
        "\n",
        "# list files in current directoty\n",
        "!ls -lah  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NpK-fo1JQUq"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# load the google word2vec model\n",
        "filename = 'GoogleNews-vectors-negative300.bin'\n",
        "model = KeyedVectors.load_word2vec_format(filename, binary=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Fl1ZQHJHGIN"
      },
      "source": [
        "#### Let's have fun"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3kxqDMymKH_"
      },
      "source": [
        "# get word vector\n",
        "model['car']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAgKecjUmW_s"
      },
      "source": [
        "# get most similar words\n",
        "model.most_similar('yellow')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bN2VuE5UmPUm"
      },
      "source": [
        "# queen = (king - man) + woman\n",
        "result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhmQmasBmi2x"
      },
      "source": [
        "# (france - paris) + spain = ?\n",
        "result = model.most_similar(positive=[\"paris\",\"spain\"], negative=[\"france\"], topn=1)\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w91o81Slm3aY"
      },
      "source": [
        "model.doesnt_match([\"red\", \"blue\", \"car\", \"orange\"])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HR-cJcT-J9Xq"
      },
      "source": [
        "## Stanford’s GloVe Embedding\n",
        "\n",
        "Like Word2Vec, the GloVe researchers also provide pre-trained word vectors. Let's download the smallest GloVe pre-trained model from the GloVe website. It's a 822 Megabyte zip file with 4 different models (50, 100, 200 and 300-dimensional vectors) trained on Wikipedia data with 6 billion tokens and a 400,000 word vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9WwmofxIIQJ"
      },
      "source": [
        "# download \n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip \n",
        "\n",
        "# unzip downloaded word embeddings\n",
        "!unzip glove.6B.zip  \n",
        "\n",
        "# list files in current directoty\n",
        "!ls -lah  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsXwAaiwI_1P"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "\n",
        "# convert the 100-dimensional version of the glove model to word2vec format\n",
        "glove_input_file = 'glove.6B.100d.txt'\n",
        "word2vec_output_file = 'glove.6B.100d.txt.word2vec'\n",
        "glove2word2vec(glove_input_file, word2vec_output_file)\n",
        "\n",
        "# load the converted model\n",
        "filename = 'glove.6B.100d.txt.word2vec'\n",
        "model = KeyedVectors.load_word2vec_format(filename, binary=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrEe17hdJEqB"
      },
      "source": [
        "# calculate: (king - man) + woman = ?\n",
        "result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}